# ğŸ—ï¸ Mini RAG â€“ Retrieval Augmented Generation Pipeline
with Hybrid Search â€¢ Citations â€¢ Highlighting â€¢ PDF Support â€¢ Streamlit UI

A production-grade Retrieval-Augmented Generation (RAG) system designed for a construction marketplace AI assistant.
The assistant answers questions strictly from internal documents using:

ğŸ” Hybrid Retrieval (BM25 + Embeddings)

ğŸ§© Document Chunking

ğŸ§  LLM Grounded Answer Generation

âœ¨ Sentence Highlighting

ğŸ“Œ Citations

ğŸ“¥ JSON Report Export

ğŸ“„ PDF/TXT Upload Support

ğŸ¨ Dark Theme UI

ğŸ—‚ï¸ Chat History

## Features

## 1. Document Chunking

Upload PDF or TXT files

Automatic PDF â†’ text conversion

Splits documents into overlapping chunks

Stored for retrieval + display

## 2. Hybrid Retrieval

Choose retrieval mode:

Mode	Description
Hybrid (BM25 + Embeddings)	Best relevance, combines keyword + semantic search
Embeddings Only	Pure semantic similarity using MiniLM
BM25 Only	Fast keyword-based retrieval
## 3. LLM Answer Generation

Uses OpenRouter to query high-quality open-source models:

Mistral-7B-Instruct

LLaMA-3-8B

Qwen-2.5-7B

DeepSeek-R1-Distill

All answers are:

grounded to retrieved text

citation-enforced

formatting-clean

âœ¨ 4. Premium UI Features

## Sentence highlighting

Collapsible chunk boxes

Dark mode

Chat history persistence

JSON report export

## Architecture
User Query
      â†“
Chunked Documents  â† PDF/TXT Upload
      â†“
Hybrid Retrieval (BM25 + Embeddings)
      â†“
Top Relevant Chunks (Highlighted)
      â†“
LLM (via OpenRouter) â€” Grounded Answer Generation
      â†“
Answer + Citations + JSON Export

## Project Structure
Mini-RAG-Pipeline/
â”‚
â”œâ”€â”€ app.py                     # Streamlit UI (premium version)
â”œâ”€â”€ rag_pipeline.py            # Core RAG logic: BM25, embeddings, hybrid search
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Documentation
â”‚
â”œâ”€â”€ /data                      # PDF/TXT documents
â”‚
â”œâ”€â”€ /outputs                   # JSON exported responses
â”‚
â””â”€â”€ /notebooks
     â””â”€â”€ evaluation.ipynb      # Optional evaluation notebook

## Installation
1. Clone the repository
git clone https://github.com/YOUR_USERNAME/Mini-RAG-Pipeline.git
cd Mini-RAG-Pipeline

2. Create & activate virtual environment

Windows

python -m venv venv
venv\Scripts\activate


Mac/Linux

python3 -m venv venv
source venv/bin/activate

3. Install requirements
pip install -r requirements.txt

4. Set OpenRouter API key

PowerShell

setx OPENROUTER_API_KEY "your_key_here"


Mac/Linux

export OPENROUTER_API_KEY="your_key_here"

### Run the RAG Application
streamlit run app.py


Then open:

ğŸ‘‰ http://localhost:8501/

## How the System Works
### 1. Upload Documents

PDF â†’ automatically converted to text

TXT â†’ loaded directly

Chunked into ~250 token windows

### 2. Retrieve Top Chunks

Depending on retrieval mode:

BM25

Embeddings

Hybrid (BM25 + Embeddings)

### 3. LLM Generates Grounded Answer

LLM is instructed to:

Use only retrieved chunks

Add citations

Avoid hallucinations

### 4. Answer Display

Highlighted matching sentences

Collapsible chunk boxes

JSON export available

## Example Query

â€œWhat factors affect construction project delays?â€

System Output Includes:

Retrieved chunk text

Highlighted matching parts

## Final answer:

Bullet points

Clean formatting

Citations (from Chunk X)

## Evaluation (Optional)

Notebook includes:

Recall@k

Relevance scoring

Hallucination detection

Model comparison

Chunk-level error analysis

## Tech Stack
Component	Tool
Embeddings	Sentence-Transformers (MiniLM-L6-v2)
Vector Search	FAISS
Keyword Search	BM25 (rank_bm25)
LLM Inference	OpenRouter API
Frontend	Streamlit
Backend	Python
ğŸ“¦ Exporting Reports

You can download:

Retrieved chunks

Final answer

Model used

Retrieval mode

Timestamp

as a .json file.

### ğŸ§‘â€ğŸ’» Developer: Shruti Thakkar

AI/ML Engineer â€” RAG â€¢ NLP â€¢ LLMs
